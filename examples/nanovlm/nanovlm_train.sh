#!/bin/bash

DATASET_PATH="/mnt/umm/users/pufanyi/workspace/Show/lmms-engine/data/llava_next.yaml"
PROCESSOR_NAME="/mnt/umm/users/pufanyi/workspace/Show/CKPT/Qwen/Qwen3-0.6B"
MODEL_PATH="/mnt/umm/users/pufanyi/workspace/Show/CKPT/Qwen/Qwen3-0.6B"
SIGLIP_PROCESSOR="/mnt/umm/users/pufanyi/workspace/Show/CKPT/google/siglip2-so400m-patch16-naflex"

ATTN_IMPLEMENTATION="flash_attention_2"
PER_DEVICE_TRAIN_BATCH_SIZE=16
LEARNING_RATE=2.0e-04
WEIGHT_DECAY=0.0
GRADIENT_ACCUMULATION_STEPS=1
GRADIENT_CHECKPOINTING=true
NUM_TRAIN_EPOCHS=1
RUN_NAME="debug_nanovlm"
OUTPUT_DIR="./output/debug_nanovlm"
WARMUP_RATIO=0.1
MAX_STEPS=10000

IMAGE_TOKEN_ID=151655

torchrun --nproc_per_node="8" \
    --nnodes="1" \
    --node_rank="0" \
    --master_addr="127.0.0.1" \
    --master_port="8000" \
    -m lmms_engine.launch.cli \
    trainer_type=fsdp2_trainer \
    dataset_config.dataset_path=${DATASET_PATH} \
    dataset_config.dataset_format=yaml \
    dataset_config.dataset_type=qwen3_vl_iterable \
    dataset_config.processor_config.processor_type=nanovlm \
    dataset_config.processor_config.processor_name=${PROCESSOR_NAME} \
    +dataset_config.processor_config.extra_kwargs.image_processor_name=${SIGLIP_PROCESSOR} \
    +dataset_config.processor_config.extra_kwargs.image_token_count=256 \
    dataset_config.packing=false \
    dataset_config.packing_strategy=first_fit \
    dataset_config.packing_length=51200 \
    dataset_config.filter_overlong=true \
    dataset_config.video_backend=qwen_vl_utils \
    dataset_config.video_sampling_strategy=fps \
    dataset_config.video_max_pixels=50176 \
    dataset_config.video_max_frames=512 \
    +model_config.load_from_config.model_type=nanovlm \
    model_config.load_from_pretrained_path=null \
    +model_config.load_from_config.config.llm_model_name=${MODEL_PATH} \
    +model_config.load_from_config.config.vision_model_name=${SIGLIP_PROCESSOR} \
    +model_config.load_from_config.config.image_token_id=${IMAGE_TOKEN_ID} \
    +model_config.load_from_config.config.vision_feature_dim=1152 \
    +model_config.load_from_config.config.image_token_count=256 \
    model_config.attn_implementation=${ATTN_IMPLEMENTATION} \
    trainer_args.freeze_modules=["visual"] \
    trainer_args.per_device_train_batch_size=${PER_DEVICE_TRAIN_BATCH_SIZE} \
    trainer_args.learning_rate=${LEARNING_RATE} \
    trainer_args.weight_decay=${WEIGHT_DECAY} \
    trainer_args.gradient_accumulation_steps=${GRADIENT_ACCUMULATION_STEPS} \
    trainer_args.gradient_checkpointing=${GRADIENT_CHECKPOINTING} \
    trainer_args.num_train_epochs=${NUM_TRAIN_EPOCHS} \
    trainer_args.warmup_ratio=${WARMUP_RATIO} \
    trainer_args.run_name=${RUN_NAME} \
    trainer_args.output_dir=${OUTPUT_DIR} \
    trainer_args.fsdp2=true \
    trainer_args.max_steps=${MAX_STEPS} \
    trainer_args.fsdp_config.transformer_layer_cls_to_wrap=["Qwen3DecoderLayer"] \
    trainer_args.fsdp_config.reshard_after_forward=false \
    trainer_args.sp_ulysses_degree=1 \
    trainer_args.use_liger_kernel=true \
    trainer_args.use_rmpad=true \
    trainer_args.dataloader_num_workers=0 \
    trainer_args.dataloader_prefetch_factor=null \
    trainer_args.print_batch_input_steps=5 \
    trainer_args.bf16=true \
    trainer_args.lr_scheduler_type=cosine \
    trainer_args.logging_steps=1 \
    trainer_args.group_by_length=false \
    trainer_args.bf16=true